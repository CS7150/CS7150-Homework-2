{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff28b42",
   "metadata": {},
   "source": [
    "# Assignment 2.1: Neural Network QuickDraw Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1d8e6",
   "metadata": {},
   "source": [
    "In this CS7150 assignment, we will develop a neural network with three fully-connected layers to perform classification, and test it out on a subset of the QuickDraw dataset. This notebook acts as a tutorial to get you started on writing Pytorch code to create Deep Learning models. \n",
    "\n",
    "**Your task**: Go through the entire notebook and fill out all the conceptual and technical questions that are indicated within the \"Exercise\" header. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c687004",
   "metadata": {},
   "source": [
    "# Setup Code\n",
    "\n",
    "Before getting started, we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the autoreload extension. This allows us to edit .py source files (if there are any), and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2cb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f340c",
   "metadata": {},
   "source": [
    "## Device Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33798935",
   "metadata": {},
   "source": [
    "We want to be able to train our model on a GPU to accelerate our computation. Let’s check to see if torch.cuda is available, else we continue to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a41951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c253a",
   "metadata": {},
   "source": [
    "## Google Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d886b1",
   "metadata": {},
   "source": [
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2338ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_colab = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    in_colab = True\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c6c81",
   "metadata": {},
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['HW2.1-QuickDrawClassifier.ipynb', 'HW2.2-CIFAR10Classifier.ipynb']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "if in_colab:\n",
    "    # TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "    # Example: If you create a CS7150 folder and put all the files under HW2 folder, then 'CS7150/HW2'\n",
    "    # GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'CS7150/HW2'\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6db073",
   "metadata": {},
   "source": [
    "# Loading TinyQuickDraw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af842fd2",
   "metadata": {},
   "source": [
    "The QuickDraw dataset is a recent popular dataset that consists of human-drawn images that were drawn by users playing the [Quick, Draw!](https://quickdraw.withgoogle.com/) game. The entire dataset consists of about 50 million drawings across 345 categories. For this exercise, we chose a subset of images, TinyQuickDraw, across 20 categories, namely: Apple, Bat, Broccoli, Carrot, Cookie, Donut, Horse, Knee, Leaf, Lobster, Mushroom, Pizza, Rain, River, Sandwich, Shark, Strawberry, T-Shirt, Van, and Watermelon. Hence, our data will be classified into one of these 20 classes.\n",
    "\n",
    "You should have a local copy of your data in the directory of where this notebook is stored. Since this is a custom dataset where images with png extensions are stored, we use the [ImageFolder](http://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) package to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "import torch.utils.data as data_utils\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "# setting directory paths for training and testing data\n",
    "url = \"https://cs7150.baulab.info/2022-Fall/data/tiny_quick_draw.zip\"\n",
    "if not os.path.isdir('tiny_quick_draw'):\n",
    "    download_and_extract_archive(url, 'tiny_quick_draw')\n",
    "train_data_dir = \"tiny_quick_draw/train\"\n",
    "test_data_dir = \"tiny_quick_draw/test\"\n",
    "train_data = ImageFolder(train_data_dir, transform=Compose([ToTensor()]))\n",
    "# storing and printing what each labels represent\n",
    "class_to_idx = train_data.class_to_idx\n",
    "print(\"Labels and their assigned label numbers: \", class_to_idx)\n",
    "test_data = ImageFolder(test_data_dir, transform=Compose([ToTensor()]))\n",
    "# check the length of dataset\n",
    "print(f'Number of samples in training data: {len(train_data)}')\n",
    "print(f'Number of samples in test data: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b2b64",
   "metadata": {},
   "source": [
    "# Displaying Loaded Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b496c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx_to_class = {}\n",
    "# creating a label number to label dict for easy representation in graphs\n",
    "for label, idx in class_to_idx.items():\n",
    "    idx_to_class[idx] = label\n",
    "    \n",
    "fig = plt.figure()\n",
    "\n",
    "# showing 6 different images of the Apple class\n",
    "for i in range(6):\n",
    "  plt.subplot(2, 3, i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Class Label: {}\".format(idx_to_class[train_data[i][1]]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b11341",
   "metadata": {},
   "source": [
    "# Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b60ba",
   "metadata": {},
   "source": [
    "Neural networks consist of modules that perform data operations. The torch.nn namespace provides all the building blocks we would need to build our own neural network. In the following sections, we’ll build a neural network to classify images in the TinyQuickDraw dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180f9c7",
   "metadata": {},
   "source": [
    "## Defining a class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e4e25",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing nn.Module, and initializing the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method. In the following cells, we show how we construct our neural network and illustrate the different components in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyQuickDrawClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyQuickDrawClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.class_size = 20\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3*28*28, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, self.class_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d758ebc",
   "metadata": {},
   "source": [
    "Let's create an instance of QuickDrawClassifier and move it to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyQuickDrawClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d94e98",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8d3f3",
   "metadata": {},
   "source": [
    "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing nn.Module automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s parameters() or named_parameters() methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640da335",
   "metadata": {},
   "source": [
    "Let’s break down the layers we created in the QuickDrawClassifier model. To illustrate it, we will take a sample image and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869873c3",
   "metadata": {},
   "source": [
    "### Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = train_data[1][0]\n",
    "input_image = input_image[None,:,:,:]\n",
    "print(\"Size of the image shown:\", input_image.size())\n",
    "plt.imshow(input_image[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Class Label: {}\".format(idx_to_class[train_data[1][1]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a59337",
   "metadata": {},
   "source": [
    "### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197f145",
   "metadata": {},
   "source": [
    "In __init__, we defined flatten to be [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html). It is used in the forward function. Its purpose is to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "fig = plt.figure()\n",
    "fig.set_figwidth(142)\n",
    "fig.set_figheight(100)\n",
    "plt.imshow(flat_image, cmap='gray') #Note: Zoom in a lot to see the image being flatten to a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff664a",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3910b3a",
   "metadata": {},
   "source": [
    "[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(in_features=3*28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290b4fd",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d81ca",
   "metadata": {},
   "source": [
    "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena. In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there’s other activations to introduce non-linearity in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41371ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5017a",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec09cd0",
   "metadata": {},
   "source": [
    "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 20)\n",
    ")\n",
    "input_image = train_data[1][0]\n",
    "logits = seq_modules(input_image[None,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f9f25c",
   "metadata": {},
   "source": [
    "### nn.Softmax and Predicting class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab940a31",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network returns logits - raw values in [-infty, infty] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. The 'dim' parameter indicates the dimension along which the values must sum to 1.\n",
    "\n",
    "*Note: Since the model hasn't been trained yet (includes randomized weights and bias values), it is very likely that the predicted image label is different from the actual one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cab7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab_detached = pred_probab.detach().numpy()[0]\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {idx_to_class[y_pred.item()]}\")\n",
    "y_actual = train_data[1][1]\n",
    "print(f\"Actual class: {idx_to_class[y_actual]}\")\n",
    "\n",
    "plt.bar([i for i in range(len(pred_probab_detached))], pred_probab_detached, color ='maroon',\n",
    "        width = 0.4)\n",
    " \n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Probability of an image being predicted as one of the class labels\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Summing of probabilities from each class for an image is equal to\", sum(pred_probab_detached))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bfbd5",
   "metadata": {},
   "source": [
    "# Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e209a3a",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is back propagation. In this algorithm, parameters (model weights) are adjusted according to the gradient of the loss function with respect to the given parameter.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph. In the following cell, we set the parameters within the model function to activate this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8601e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925f9d9",
   "metadata": {},
   "source": [
    "Before defining our training loop, let's define our hyperparameters, loss and optimizer functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd6c1d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa834c",
   "metadata": {},
   "source": [
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates.\n",
    "\n",
    "We define the following hyperparameters for training:\n",
    "\n",
    "- Number of Epochs - the number times to iterate over the dataset\n",
    "\n",
    "- Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
    "\n",
    "- Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80fd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "batch_size = 500\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622f798",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb140687",
   "metadata": {},
   "source": [
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value. Common loss functions include [nn.MSELoss (Mean Square Error)](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) and nn.CrossEntropyLoss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f21dc",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be187fdb",
   "metadata": {},
   "source": [
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this case, we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different [optimizers](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4dbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6282d",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70932b79",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "\n",
    "- Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "\n",
    "- Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "- Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a06d08",
   "metadata": {},
   "source": [
    "In the above cell, we used a Dataloader to create batches for training and testing data. For each batch of size indicated in the batch_size hyperparameter, we perform backprop and update the model parameters' weights and biases.\n",
    "\n",
    "In the following cell, we define our train_loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, print_log=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "    training_acc = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred, y.to(device))\n",
    "        correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (print_log==True) and (batch % 100 == 0):\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"\"\"Training loop: loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\"\"\")\n",
    "    correct /= size\n",
    "    training_acc = 100*correct\n",
    "    if (print_log==True):\n",
    "        print(f\"\"\"Training Accuracy: {training_acc:>0.1f}%\"\"\")\n",
    "    return training_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788c657",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1069ff6",
   "metadata": {},
   "source": [
    "In the test loop, we iterate over the test dataset to check if model performance is improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, print_log=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            test_loss += loss_fn(pred, y.to(device)).item()\n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    testing_acc = 100*correct\n",
    "    if (print_log==True):\n",
    "        print(f\"Testing loop: \\n Accuracy: {testing_acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return testing_acc, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b66834",
   "metadata": {},
   "source": [
    "## Running the loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499d58d",
   "metadata": {},
   "source": [
    "We run our loops for a certain number of times, which is indicated in the 'epoch' hyperparameter that we defined earlier. In the following cell, we run both our training and testing loop to see how our training and testing accuracies change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01bf15a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_train_accs = []\n",
    "example_test_accs = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    training_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing_acc, _ = test_loop(test_dataloader, model, loss_fn)\n",
    "    example_train_accs.append(training_acc)\n",
    "    example_test_accs.append(testing_acc)\n",
    "print(\"Done!\")\n",
    "\n",
    "epoch_list = [i for i in range(epochs)]\n",
    "plt.plot(epoch_list, example_train_accs, color ='tab:green', label='training accuracy')\n",
    "plt.plot(epoch_list, example_test_accs, color ='tab:blue', label='testing accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b237a77",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f296f",
   "metadata": {},
   "source": [
    "When the model is not trained properly, there are generally two extreme phenonemons that can occur. One is that the model is underfitted, which means that the model has not captured the underlying logic of the data. We would have high loss and low accuracy in such a case.\n",
    "\n",
    "In the above trained model, you will notice that the training accuracy is around 100% while the testing accuracy is around 50%. When there is such a significant difference between these accuracies, we say that the model is overfitted. This means that our training has focused on the particular training set so much that the model has missed its classification objective entirely. We don't want that to happen!\n",
    "\n",
    "**Exercise**: We would like you raise the testing accuracy until it is close to the training accuracy. We have a code block after the following section for you to demonstrate final modified model and its hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a5c7e",
   "metadata": {},
   "source": [
    "# Understanding Autograd Better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4ab3a",
   "metadata": {},
   "source": [
    "The autograd has a pack_hook function, which is called everytime an operation saves a tensor for backward.\n",
    "\n",
    "In the following cell, we print out the functions for which respective tensors are stored for computational purposes in one backward pass. We defined two functions, pack_hook and unpack_hook. The output of pack_hook is stored in the computation graph instead of the original tensor. The unpack_hook uses that return value to compute a new tensor, which is the one actually used during the backward pass.  In general, you want unpack_hook(pack_hook(t)) to be equal to t. Under the hood, PyTorch has **packed** and **unpacked** the tensor to prevent reference cycles. As a rule of thumb, you should not rely on the fact that accessing the tensor saved for backward will yield the same tensor object as the original tensor. They will however share the same storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_hook(x):\n",
    "    print(\"Packing/Storing following value in computation graph\", x.grad_fn, x.shape)\n",
    "    return x\n",
    "\n",
    "def unpack_hook(x):\n",
    "    print(\"Unpacking/Creating following tensor for backward pass\", x.grad_fn, x.shape)\n",
    "    return x\n",
    "\n",
    "def grad_hook(x):\n",
    "    print(\"Updating gradient of\", x.grad_fn, x.shape)\n",
    "    return x\n",
    "\n",
    "def train_instance_with_saved_tensor(dataloader, model, loss_fn, optimizer, print_log=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    for p in model.parameters():\n",
    "        p.register_hook(grad_hook)\n",
    "    with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            print(\"About to run the model on a batch of size\", len(X))\n",
    "            pred = model(X.to(device))\n",
    "            print(\"About to run the loss\")\n",
    "            loss = loss_fn(pred, y.to(device))\n",
    "\n",
    "            # Backpropagation\n",
    "            print(\"About to set zero grad\")\n",
    "            optimizer.zero_grad()\n",
    "            print(\"loss.backward about to run\")\n",
    "            loss.backward()\n",
    "            print(\"optimizer.step() about to run\")\n",
    "            optimizer.step()\n",
    "            break\n",
    "                \n",
    "                \n",
    "print(f\"Running one backpass\")\n",
    "train_instance_with_saved_tensor(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f1a89",
   "metadata": {},
   "source": [
    "## Exercise 2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e8de3",
   "metadata": {},
   "source": [
    "**Question:** Based on the saved_tensor_hook results, what values are stored for backward pass and why? What values are updated in one backward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ee091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:Replace the placeholder text and fill out the print statement below to answer the above question\n",
    "print(\"\"\"TO DO: Answer the above question. \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0389ff2",
   "metadata": {},
   "source": [
    "## Exercise 2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b815c24",
   "metadata": {},
   "source": [
    "**Exercise**: As mentioned earlier, we would like you to reduce the overfitting of the model. Overfitting is caused when the model is overparameterized compared to the size and diversity of the training data, and can be improved in the following ways:\n",
    "\n",
    " * Increase the size of the training data.\n",
    " * Reduce the parameters of the model, or change the number of layers.\n",
    " * Increase regularization of the model, for example, by using weight decay.\n",
    "\n",
    "In the code below we have already done the first of those steps: we have increased the size of the training data.  Be sure to shuffle the batches of training data to ensure diversity.\n",
    "\n",
    "You can further reduce overfitting by changing the number of neurons in in your model.  One way to reduce the number of neurons while keeping an expressive model is to arrange fewer neurons in more layers.\n",
    "\n",
    "You can also reduce overfitting by adjusting the hyperparameters.  Weight decay reduces the size of parameters and encourages the model to learn a simpler function that will be less prone to overfitting.\n",
    "\n",
    "In the code below we have defined the a new class `TinyQuickDrawStudentClassifier` for the  classifier and redefined the hyperparameters. You should re-implement this class and change the hyperparameters until you get the testing accuracy to stabilize at 60\\% or above.  It is OK for training accuracy to be low; your aim should be to have training accuracy close to testing accuracy.\n",
    "\n",
    "You can use the **Fine-Tuning Hyperparameters\"** section to understand and record how changes to different parameter values affect your model's accuracy.\n",
    "\n",
    "To get full credit for this section, the testing accuracy should be 60% or above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5534080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyQuickDrawStudentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyQuickDrawStudentClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.class_size = 20\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3*28*28, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, self.class_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1521d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 500\n",
    "epochs = 300\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "dampening = 0\n",
    "\n",
    "\n",
    "# Train and Test\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "training_accs = []\n",
    "model = TinyQuickDrawStudentClassifier().to(device)\n",
    "model.requires_grad_(True)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                        lr = learning_rate, \n",
    "                        momentum = momentum, \n",
    "                        weight_decay = weight_decay, \n",
    "                        dampening= dampening)\n",
    "# Loss Func\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Dataloaders\n",
    "# Created a bigger training dataset for better model training\n",
    "train_data_dir = \"tiny_quick_draw/bigger_train\"\n",
    "train_data = ImageFolder(train_data_dir, transform=Compose([ToTensor()]))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "example_train_accs = []\n",
    "example_test_accs = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    training_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    testing_acc, _ = test_loop(test_dataloader, model, loss_fn)\n",
    "    example_train_accs.append(training_acc)\n",
    "    example_test_accs.append(testing_acc)\n",
    "print(\"Done!\")\n",
    "\n",
    "epoch_list = [i for i in range(epochs)]\n",
    "plt.plot(epoch_list, example_train_accs, color ='tab:green', label='training accuracy')\n",
    "plt.plot(epoch_list, example_test_accs, color ='tab:blue', label='testing accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714562c6",
   "metadata": {},
   "source": [
    "# Fine-Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385fec6",
   "metadata": {},
   "source": [
    "**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get some practice with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3656b",
   "metadata": {},
   "source": [
    "## Exercise 2.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fc4ff",
   "metadata": {},
   "source": [
    "Below, you should experiment with different values of the various hyperparameters, including layer size, batch size, learning rate, and number of training epochs. You should also experiment with SGD optimization hyperparameters including momentum, weight decay and more. Plot at least 3 graphs to illustrate how does the change of one hyperparameter (choose any except for epochs) affect the training and testing accuracy and testing loss. To show these plots, you can add more code blocks in this section. We provide a boilerplate code for one (epochs) to give an idea of what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50ab30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = [1, 5, 10, 15, 20, 25]\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "dampening = 0\n",
    "\n",
    "# Train and Test\n",
    "test_accs = []\n",
    "test_losses = []\n",
    "training_accs = []\n",
    "for e in epochs: #Would change this to reflect whatever hyperparameter you would be testing\n",
    "    # Model\n",
    "    model = TinyQuickDrawStudentClassifier().to(device)\n",
    "    model.requires_grad_(True)\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr = learning_rate, \n",
    "                            momentum = momentum, \n",
    "                            weight_decay = weight_decay, \n",
    "                            dampening= dampening)\n",
    "    # Loss Func\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Dataloaders\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "    final_train_acc = 0\n",
    "    final_test_acc = 0\n",
    "    final_test_loss = 0\n",
    "    for t in range(e):\n",
    "        print(f\"Currently running epoch {t+1}\")\n",
    "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
    "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
    "        final_test_acc = testing_acc\n",
    "        final_test_loss = test_loss\n",
    "        final_train_acc = training_acc\n",
    "    test_accs.append(final_test_acc)\n",
    "    test_losses.append(final_test_loss) \n",
    "    training_accs.append(final_train_acc)\n",
    "plt.plot(epochs,test_losses, color ='tab:red', label='testing loss')\n",
    "plt.plot(epochs,test_accs, color ='tab:blue', label='testing accuracy')\n",
    "plt.plot(epochs,training_accs, color ='tab:green', label='training accuracy')\n",
    "plt.legend()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0433fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
