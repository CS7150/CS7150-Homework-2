{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1h9cNT1gWQOD",
      "metadata": {
        "id": "1h9cNT1gWQOD"
      },
      "source": [
        "<font size='6'>**Homework 2.1: Training a Deep Network**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "990bbb32",
      "metadata": {
        "id": "990bbb32"
      },
      "source": [
        "# Overview\n",
        "\n",
        "We will start by exploring the optimization aspects of deep network training. Throughout this journey, you will gain insights into:\n",
        "\n",
        "**Part 1:**\n",
        "- The fundamentals of simple gradient descent.\n",
        "- The concept of weight decay.\n",
        "- A deep understanding of PyTorch autograd and PyTorch optimizers.\n",
        "\n",
        "**Part 2:**\n",
        "- Analyzing raw gradients, means, and RMS (Root Mean Square).\n",
        "- Delving into exponential moving averages.\n",
        "- Exploring the workings of the ADAM optimization algorithm.\n",
        "\n",
        "**Part 3:**\n",
        "- Strategies for optimizing neural network parameters.\n",
        "- Selecting appropriate nonlinearities, architectures, and layers to tackle the vanishing gradient problem.\n",
        "- Leveraging techniques like regularization, parameterization, and specific layer choices to enhance generalization.\n",
        "- Unpacking the roles and impacts of ADAM, ReLU activation, weight decay, network depth, network width, residual architectures, and batch normalization in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XhztdqIaXIND",
      "metadata": {
        "id": "XhztdqIaXIND"
      },
      "source": [
        "#Note\n",
        "\n",
        "<font size='4'>  \n",
        "- **You do not need to tune hyper parameters in the regular tasks.**\n",
        "\n",
        "- **You do not install any additional packages inside the Colab environment.**  \n",
        "\n",
        "- **If you collaborate or get assistance from classmates, online resources, AI, or other sources, then you must then you must explicitly write down the sources that you used to credit them.**\n",
        "\n",
        "- **Attend office hours and make post on Piazza if you have any questions.**\n",
        "\n",
        "- **You have sufficient time to work on this assignment. Please refrain from asking for extensions.**\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e753",
      "metadata": {
        "id": "01e9e753"
      },
      "source": [
        "##<font size='5'>**Setup**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014ada4e",
      "metadata": {
        "id": "014ada4e"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "from torch.nn import Sequential, Module\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from hw2utils import LossFunctionWithPlot, ConstantVectorNetwork"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Uf6xtuLHB3wc",
      "metadata": {
        "id": "Uf6xtuLHB3wc"
      },
      "source": [
        "##<font size='5'>**Part 1: Simple Gradient Descent**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hiUTm2E_xdq3",
      "metadata": {
        "id": "hiUTm2E_xdq3"
      },
      "source": [
        "Gradient descent is an iterative optimization algorithm for finding the minimum of a function. It works by starting at a point and then moving in the direction of the steepest descent until it reaches a minimum. The steepest descent is the direction in which the function is decreasing most rapidly.\n",
        "\n",
        "To train a model using gradient descent, we start with a set of initial parameters. These parameters are the values of the variables in the model. We then repeatedly apply gradient descent to update the parameters. Each update moves the parameters in the direction of the steepest descent. This continues until the parameters converge to a minimum of the function.\n",
        "\n",
        "The choice of the learning rate is important for gradient descent. The learning rate is the size of the steps that are taken in the direction of the steepest descent. If the learning rate is too small, the algorithm will converge slowly. If the learning rate is too large, the algorithm may diverge and never converge.\n",
        "\n",
        "Let's say we have a function $\\mathcal{L}(x)$ that we want to minimize. The gradient of $\\mathcal{L}(x)$ is a vector that points in the direction of the steepest descent of $f(x)$. The gradient can be calculated using the following equation:\n",
        "\n",
        "\\begin{align*}\n",
        "\\nabla \\mathcal{L}(x) &= \\begin{bmatrix}\n",
        "\\frac{\\partial \\mathcal{L}(x)}{\\partial x_1} \\\\\n",
        "\\frac{\\partial \\mathcal{L}(x)}{\\partial x_2} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial \\mathcal{L}(x)}{\\partial x_n}\n",
        "\\end{bmatrix} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "The gradient descent algorithm can be used to minimize f(x) by repeatedly taking steps in the direction of the gradient. The update rule for gradient descent is given by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "x_\\text{new} = x_\\text{old} - \\alpha \\cdot \\nabla \\mathcal{L}(x_\\text{old})\n",
        "\\end{equation}\n",
        "\n",
        "where $x_\\text{old}$ is the current value of $x$, $x_\\text{new}$ is the new value of $x$, $\\alpha$ is the learning rate, and $\\nabla \\mathcal{L}(x_\\text{old})$ is the gradient of $\\mathcal{L}(x)$ evaluated at $x_\\text{old}$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RvUUakmqt9Jr",
      "metadata": {
        "id": "RvUUakmqt9Jr"
      },
      "source": [
        "<font size='4'>**Simple Implementation of Gradient Descent on a quadratic loss**</font>\n",
        "\n",
        "Below we demonstrate gradient descent optimization in action.\n",
        "\n",
        "We iteratively update the `x` to try to minimize a quadratic function `L` defined by the `LossFunctionWithPlot` class. The trajectory of updates and corresponding losses are stored and plotted to visualize the optimization progress.\n",
        "\n",
        "Provide an implementation of simple gradient descent below.  In 21 steps you can make the loss go down to about 3 or better, and drive `x` somewhat towards the center of the minimum of the loss function.\n",
        "\n",
        " * Use `loss.backward()` (read https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html)\n",
        " * Use `x.grad` to get the gradient.\n",
        " * Update `x` in-place using `x -= `*something*, and know why `torch.no_grad()` is needed.\n",
        " * Understand why gradients need to be zeroed: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y93nIjxHcbmW",
      "metadata": {
        "id": "y93nIjxHcbmW"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.1 - Implement simple gradient descent ( 1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f0c5514",
      "metadata": {
        "id": "6f0c5514"
      },
      "outputs": [],
      "source": [
        "# Do not change the starting x.\n",
        "x = torch.tensor([-1.5, 1.2])\n",
        "x.requires_grad = True\n",
        "L = LossFunctionWithPlot()\n",
        "\n",
        "# Start by using a learning rate of 0.1.\n",
        "learning_rate = 0.1\n",
        "\n",
        "for iter in range(50):\n",
        "    loss = L(x)\n",
        "    if iter % 7 == 0: print(f'Loss at step {iter} is {loss.item():.3f}')\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "    ############################################################################\n",
        "    # TODO: Implement Simple Gradient Descent and update variable 'x'\n",
        "    # Read Documentation to compute a gradient of a parameter -\n",
        "    # https://pytorch.org/docs/stable/autograd.html\n",
        "    ############################################################################\n",
        "    x=None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    x.grad = None\n",
        "L.plot_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xj3S0GHWFshy",
      "metadata": {
        "id": "Xj3S0GHWFshy"
      },
      "source": [
        "One reason it is hard for gradient descent to move towards the bottom of the bowl is that there is no single ideal learning rate for all dimensions.  In the problem in 1.1, notice that:\n",
        " * the loss changes direction very quickly (the curvature is high) in the horiztonal direction of `x[0]`\n",
        " * the loss is very slow-changing (the curvature is lower) in the vertical direction of `x[1]`.\n",
        "\n",
        "The \"sawtooth\" loss curves and the \"zig-zag\" paths are symptoms of an optimizer that is taking steps that are too large: the path could be repeatedly jumping over a valley in the loss surface and ending up at another point of high, or even higher loss.  A lower learning rate can help, but it can lead to another problem. (What problem?  Try it.)\n",
        "\n",
        "Another fancy idea is to mix learning rates, with different learning rates for each parameter.  Although you can still make a learning rate too high or too low."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BeC54gE4dJX8",
      "metadata": {
        "id": "BeC54gE4dJX8"
      },
      "source": [
        "<font size='4' color='red'> Task 1.2 - Explore Simple Gradient Descent using various learning rates (_ points)</font>\n",
        "\n",
        "Now copy your code from 1.1 below here, but **experiment with learning rates**, including **unequal learning rates** for `x[0]` and `x[1]` by setting to a tensor with two values.  Try to find a pair of learning rates that move $x$ to the bottom of the bowl and stays there with near-zero loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1bb536b7",
      "metadata": {
        "id": "1bb536b7"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO: Copy your solution from Task 1.1 here and attempt to discover a pair of\n",
        "# learning rates that guide the optimization process to place 'x' at the lowest\n",
        "#point of the bowl and maintain it there with a nearly zero loss.\n",
        "################################################################################\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4' color='red'>**Inline Question 1.A ( 1 point ):** What bad things tend to happen when the learning rate is too high? </font>\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "$$\\text{When the learning rate is too high } \\fbox{write what tends to happen}$$"
      ],
      "metadata": {
        "id": "I2fobwBkxhUQ"
      },
      "id": "I2fobwBkxhUQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4' color='red'>**Inline Question 1.B ( 1 point ):**  What bad things tend to happen when the learning rate is too low?</font>\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "$$\\text{When the learning rate is too low } \\fbox{write what tends to happen}$$"
      ],
      "metadata": {
        "id": "z-fRCfY5xrYe"
      },
      "id": "z-fRCfY5xrYe"
    },
    {
      "cell_type": "markdown",
      "id": "e5574269",
      "metadata": {
        "id": "e5574269"
      },
      "source": [
        "<font size='4' color='red'>**Inline Question 1.C (1 point):**  Should a higher or lower learning rate be used on a dimension with higher curvature (with sharper changes)?</font>\n",
        "\n",
        "**Answer**:\n",
        "\n",
        "$$\\text{When the dimension has higher curvature, the learning rate should generally be } \\fbox{fill in higher or lower}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B9CrZHsUOEll",
      "metadata": {
        "id": "B9CrZHsUOEll"
      },
      "source": [
        "<font size='4'>**II) Weight Decay, aka L2 regularization**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0QAUkEMPKa5b",
      "metadata": {
        "id": "0QAUkEMPKa5b"
      },
      "source": [
        "Weight decay, aslo known as L2 regularization, adds the following term to the total loss:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}(\\theta) = \\mathcal{L}_\\text{objective}(\\theta) + \\frac{\\lambda}{2\\nu} |\\theta|^2\n",
        "\\end{equation}\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\mathcal{L}(\\theta)$ - is the regularized objective function.\n",
        "- $\\mathcal{L}_\\text{objective}(\\theta)$ - is the original objective function (without regularization).\n",
        "- $\\theta$ is the vector of model parameters.\n",
        "- $\\lambda$ is the regularization parameter.\n",
        "- $\\nu$ is the learning rate being used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ON1sjGghg3cD",
      "metadata": {
        "id": "ON1sjGghg3cD"
      },
      "source": [
        "<font size='4' color='red'>**1.D) Inline Question (1 point):** What is the gradient $\\nabla L(\\theta)$ in terms of $\\nabla \\mathcal{L}_\\text{objective}(\\theta)$ and $\\theta$?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb_LLah_MSHs",
      "metadata": {
        "id": "gb_LLah_MSHs"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$\n",
        "\n",
        "\\begin{equation}\n",
        "Ans. \\nabla L(\\theta) = \\nabla L_{objective}(\\theta) + λ/ν * \\theta\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-lEo3NSYMULF",
      "metadata": {
        "id": "-lEo3NSYMULF"
      },
      "source": [
        "<font size='4' color='red'>**1.E) Inline Question (1 point):** Suppose 'ν' is the learning rate. Then what should be the update rule for Θ?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y5bUtU7aOMPE",
      "metadata": {
        "id": "y5bUtU7aOMPE"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$\n",
        "\n",
        "\\begin{equation}\n",
        "Ans. \\theta = \\theta - (ν * \\nabla L_{objective}(Θ) + λ* θ)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8f05372",
      "metadata": {
        "id": "c8f05372"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.3 - Implement a `SimpleGradientDescent` optimizer class in pytorch (2 point)</font>\n",
        "\n",
        "\n",
        "Pytorch encapsulates optimization algorithms into optimizer classes that hold on to a list of parameters being optimized, and that update the parameters in-place based on gradients using a `step()` method.\n",
        "\n",
        "Here we see how that pattern works.\n",
        "\n",
        "Implement the `SimpleGradientDescent` class as a pytorch-style optimizer by completing the code below.\n",
        "\n",
        "Incorporate weight decay by adding the term you computed above, where $\\lambda$ is the `weight_decay` hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kbNId-rwFpPj",
      "metadata": {
        "id": "kbNId-rwFpPj"
      },
      "outputs": [],
      "source": [
        "class SimpleGradientDescent():\n",
        "    def __init__(self, parameters, lr=0.1, weight_decay=0.0):\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.parameters = []\n",
        "        for x in parameters:\n",
        "            self.parameters.append(x)\n",
        "    def step(self):\n",
        "      with torch.no_grad():\n",
        "            for x in self.parameters:\n",
        "        ########################################################################\n",
        "        # TODO:  Implement Simple Gradient Descent with the inclusion of\n",
        "        # weight_decay, and then update the results in variable x.\n",
        "        ########################################################################\n",
        "                x=None\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "    def zero_grad(self):\n",
        "        for x in self.parameters:\n",
        "            x.grad = None\n",
        "\n",
        "x = torch.tensor([-1.5, 1.2])\n",
        "x.requires_grad = True # This tells PyTorch that the x variable will be used to calculate gradients.\n",
        "\n",
        "L = LossFunctionWithPlot()\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = SimpleGradientDescent([x], lr=learning_rate, weight_decay=1e-3)\n",
        "\n",
        "for iter in range(50):\n",
        "    loss = L(x)\n",
        "    if iter % 7 == 0: print(f'Loss at step {iter} is {loss.item():.3f}')\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "L.plot_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-_U2gL0uk499",
      "metadata": {
        "id": "-_U2gL0uk499"
      },
      "source": [
        "##<font size='5'>**Part 2: The ADAM optimizer**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4'>**I) Analyzing Raw Gradients, Means, and RMS (Root Mean Square).**</font>"
      ],
      "metadata": {
        "id": "WoAgbuAb2ioK"
      },
      "id": "WoAgbuAb2ioK"
    },
    {
      "cell_type": "markdown",
      "id": "_VpedU2flT6m",
      "metadata": {
        "id": "_VpedU2flT6m"
      },
      "source": [
        "<font size='4'>**Measure mean of the gradient on each dimension**</font>\n",
        "\n",
        "First, let us plot and examine the mean of the gradient.\n",
        "\n",
        "Using the code below as a starting point, calculate and fill in the following mean gradient over the 50 iterations in the specific optimization below:\n",
        "\n",
        "$$\\mathtt{mean\\_grads[i]} = \\frac{1}{N} \\sum_{t=1}^N \\frac{\\partial \\mathcal{L}}{\\partial x_i}(x^{(t)})$$\n",
        "\n",
        "<font size='4' color='red'>**2.A) Inline Questions (2 points):**</font>\n",
        "\n",
        "$$\\text{1. Mean gradient component with respect to x[0]} = \\fbox{ TODO: what number }$$\n",
        "$$\\text{2. Mean gradient component with respect to x[1]} = \\fbox{ TODO: what number }$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ouszwPql2q1",
      "metadata": {
        "id": "5ouszwPql2q1"
      },
      "source": [
        "<font size='4'>**Measure root-mean-square of the gradient on each dimension**</font>\n",
        "\n",
        "Second, let's plot the root mean square (RMS) of the gradient.\n",
        "\n",
        "Using the code below as a starting point, calculate and fill in the following root-mean-square gradient over the 50 iterations in the specific optimization below:\n",
        "\n",
        "$$\\mathtt{rms\\_grads[i]} = \\frac{1}{N} \\sqrt{\\sum_{t=1}^N \\left(\\frac{\\partial \\mathcal{L}}{\\partial x_i}(x^{(t)})\\right)^2}$$\n",
        "\n",
        "<font size='4' color='red'>**2.B) Inline Questions (2 points):**</font>\n",
        "\n",
        "$$\\text{ 1. RMS gradient component with respect to x[0]} = \\fbox{ TODO: what number }$$\n",
        "\n",
        "$$\\text{ 2. RMS gradient component with respect to x[1]} = \\fbox{ TODO: what number }$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zx8lRDHblALC",
      "metadata": {
        "id": "Zx8lRDHblALC"
      },
      "source": [
        "\n",
        " <font size='4'>**Understanding the challanges faced by simple gradient descent.**</font>\n",
        "\n",
        "The optimization problem in Part I causes gradient descent to run into a few different problems:\n",
        " * after initially descending  the loss jumps back up.\n",
        " * after making initial quick progress, $x$ gets stuck instead of heading towards the middle."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4' color='Red'>Task 2.1 Insights from Raw Gradient Plots and Statistical Metrics (2 points)</font>\n",
        "\n",
        "To understand the problems, run the code below to see plots of the raw gradient, and then compute the Mean and root-mean-square (RMS) of each component of the gradient over all the iterations.\n"
      ],
      "metadata": {
        "id": "BY9b9lgh1BRq"
      },
      "id": "BY9b9lgh1BRq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31bd2478",
      "metadata": {
        "id": "31bd2478"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([-1.5, 1.2])\n",
        "x.requires_grad = True # This tells PyTorch that the x variable will be used to calculate gradients.\n",
        "\n",
        "L = LossFunctionWithPlot()\n",
        "\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD([x], lr=learning_rate)\n",
        "grads = []\n",
        "\n",
        "for iter in range(50):\n",
        "    loss = L(x)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        optimizer.step()\n",
        "    grads.append(x.grad.clone())\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "grads = torch.stack(grads)\n",
        "\n",
        "###################################################################################\n",
        "# TODO: compute mean derivatives and root mean square derivatives for x[0] and x[1]\n",
        "###################################################################################\n",
        "\n",
        "mean_grads = None # should be computed as a pair of means, one for each dimesnion\n",
        "rms_grads = None # should be computed as a pair of root-mean-squares, one for each dimension\n",
        "\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################\n",
        "\n",
        "print('Mean grads for two dimensions separately:', mean_grads)\n",
        "print('Root mean square grads for two dimensions separately:', rms_grads)\n",
        "\n",
        "L.plot_history(grads=grads, mean_grads=mean_grads, rms_grads=rms_grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62598CcSmlyL",
      "metadata": {
        "id": "62598CcSmlyL"
      },
      "source": [
        "<font size='4'>**Facts about mean and RMS**</font>\n",
        "\n",
        "To understand the role of the mean and RMS, answer the following questions.\n",
        "\n",
        "<font size='4' color='red'>**2.C) Inline Question (4 points):**</font>\n",
        "\n",
        "$$\\text{1. When they differ, which is guaranteed to be smaller: mean or RMS?} = \\fbox{ TODO: say which }$$\n",
        "\n",
        "$$\\text{2. Which is a better representation of the \"average size\", mean or RMS?} = \\fbox{ TODO: say which }$$\n",
        "\n",
        "$$\\text{3. When the derviative is consistently positive, will the mean tend to be larger or smaller?} = \\fbox{ TODO: larger or smaller }$$\n",
        "\n",
        "$$\\text{4. When the derviative sign changes frequently, will the mean tend to be larger or smaller?} = \\fbox{ TODO: larger or smaller }$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RjTlBbV9ov9_",
      "metadata": {
        "id": "RjTlBbV9ov9_"
      },
      "source": [
        "<font size='4'>**Anticipating problems in optimization using Mean and RMS**</font>\n",
        "\n",
        "Based on the problems we are seeing, we want the optimizer to slow down when the mean is much smaller than the RMS, and speed up when the mean and RMS are about the same size.  That will:\n",
        "\n",
        "  1. Make sure the updates to be **small enough** when the gradient starts becoming bumpy, instead of oscillating.\n",
        "  2. Make sure the updates to be **large enough** when gradient is smooth but happens to be small.\n",
        "  \n",
        "The mean and RMS of the gradient can be used to deal with both these problems.  In the next section, we will see how they are directly applied in the ADAM optimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb61e28",
      "metadata": {
        "id": "1eb61e28"
      },
      "source": [
        "<font size='4'>**The idea of the ADAM optimizer**</font>\n",
        "\n",
        "The ADAM optimizer automatically chooses a different learning rate for each parameter by using a heuristic that shrinks the update size in regions where the gradient is changing more quickly, while normalizing the update size so that it is a consistent size even in regions where the gradient is very small.  Update magnitudes are calculated per-parameter, so ADAM can help deal with parameters that behave very differently from each other.\n",
        "\n",
        "\n",
        "The idea behind ADAM is to choose an update that is proportional to a fraction between a weighted mean of the gradient and a weighted RMS of the gradient:\n",
        "\n",
        "$$\\Delta x = - \\alpha \\frac{\\text{mean gradient}}{\\text{rms gradient}} = - \\alpha \\frac{\\sum_i w_i g_i}{\\sqrt{\\sum_i u_i g_i^2}}$$\n",
        "\n",
        "In the definition above, the $g_i$ are samples of the gradient from previous steps, and $w_i$ and $u_i$ are the weights to use for averaging.\n",
        "\n",
        "Why one might divide the mean gradient by the RMS of the gradient?\n",
        "\n",
        "To understand, answer the following question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i6Oo-nPGqrr4",
      "metadata": {
        "id": "i6Oo-nPGqrr4"
      },
      "source": [
        "<font size='4' color='red'>**2.D) Inline Question (2 points):**\n",
        "\n",
        "Suppose there is a new problem which is scaled by some constant $K$ so that all the new gradients are uniformly scaled larger. We want to understand whether ADAM speeds up or slows down when gradients are scaled up. Precisely: If gradients $\\hat{g}_i=Kg_i$ are scaled up with $K>1$, how will the ADAM update $\\Delta\\hat{x}$ relate to the original problem's ADAM update $\\Delta x$ i?  (e.g., which is larger?) </font>\n",
        "\n",
        "**Answer.**\n",
        "\n",
        "$$\\fbox{ TODO: say which is larger, if any, and prove why. }$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y30Vk-QUqtAu",
      "metadata": {
        "id": "Y30Vk-QUqtAu"
      },
      "source": [
        "This property means that ADAM will not go too fast nor too slow just because the average size of the gradient is too large.  The only thing that will cause ADAM to slow down is when the mean is much smaller than the RMS, which happens when the gradient frequently changes sign, for example, when the optimizer is oscillating around a minima.\n",
        "\n",
        "Because the mean and RMS will change during the optimization, in practice the ADAM algorithm is based on using **exponential moving averages** which will adapt as optimization proceeds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf3G1ssXvach",
      "metadata": {
        "id": "bf3G1ssXvach"
      },
      "source": [
        "<font size='4'>**II) Exponential Moving Average (EMA) on Time Series Data**</font>\n",
        "\n",
        "The exponential Moving Average (EMA) represents a moving average variant that assigns greater importance to the most recent data points within a time series. It achieves this by progressively diminishing the influence of older data points. Unlike simple moving averages, where all data points hold the same weight, EMA's differential weighting scheme enhances its sensitivity to recent data alterations, rendering it highly attuned to the latest fluctuations within the data.\n",
        "\n",
        "EMA is a weighted average where, the weight of a sample of age $t-i$ is decayed exponentially by $\\beta^{(t-i)}$, where $\\beta < 1$ is the smoothing parameter.  That is, using geometric series identities,\n",
        "\n",
        "$$\\text{EMA}_t = \\frac{\\beta^{t-1} x_1 + \\beta^{t-2} x_1 + ... + \\beta x_{t-1} + x_t}{\\beta^{t-1} + \\beta^{t-2} + ... + \\beta + 1} = \\frac{(1 - \\beta) \\cdot \\sum_i \\beta^{t-i}x_i}{1 - \\beta^{t}}$$\n",
        "\n",
        "As $t$ gets large, the denominator becomes indistinguishable from one, and EMA can be estimated by computing just the numerator $\\text{EMA}_t^* = (1 - \\beta) \\cdot \\sum_i \\beta^{t-i}x_i$.  The numerator has the advantage that maintaining a running average only requires a single number be remembered: the most recent numerator $\\text{EMA}_{t-1}^*$.  The formula for calculating the numerator $\\text{EMA}_t^*$ is as follows:\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{EMA}_0^* &= 0 \\\\\n",
        "\\text{EMA}_t^* &= \\beta \\cdot \\text{EMA}_{t-1}^* + (1 - \\beta) \\cdot x_t\n",
        "\\end{align*}\n",
        "\n",
        "When $t$ is small, the numerator can be much smaller than one, so it must be included, so the full formula for the EMA is:\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{EMA}_t = \\frac{\\text{EMA}_t^*}{1 - \\beta^{t}}\n",
        "\\end{align*}\n",
        "\n",
        "Where,\n",
        "\n",
        "- $\\text{EMA}_t$ - is the Exponential Moving Average at time t.\n",
        "- $\\text{EMA}_t^*$ - is the Exponential Moving Average Numerator, which $\\approx \\text{EMA}_t$ when $t$ is large.\n",
        "- $x_t$ - is the data point at time t that you want to include in the EMA calculation.\n",
        "- $\\text{EMA}_{t−1}^*$ - is the EMA numerator calculated at the previous time step t−1.\n",
        "- $\\beta$ is the smoothing factor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2171690",
      "metadata": {
        "id": "f2171690"
      },
      "source": [
        "<font size='4' color='Red'>Task 2.2 Implement EMA (2 points)</font>\n",
        "\n",
        "Based on the definitions above, implement ema_update below, and produce the plot of the EMA of the synthetic time series data.\n",
        "\n",
        "As you can see, unlike the orindary mean, EMA adapts to changes in the data over time.\n",
        "\n",
        "Also notice the difference between EMA* and EMA at the beginning of the series.  Notice that EMA* is not unbiased: instead it has a clear bias towards zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d04d63bf",
      "metadata": {
        "id": "d04d63bf"
      },
      "outputs": [],
      "source": [
        "def ema_update(x_t, beta, t, ema_star_old):\n",
        "    ema_star_t = 0.0\n",
        "    ema = 0.0\n",
        "    ############################################################################\n",
        "    # TODO: 1) compute ema_star_t from ema_star_old, beta, and x_t.\n",
        "    #       2) compute ema from ema_star_t, beta, and t\n",
        "    ############################################################################\n",
        "\n",
        "    ema_star_t = None\n",
        "    ema = None\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "    return ema, ema_star_t\n",
        "\n",
        "timestamps = 100\n",
        "time_series = torch.cat([\n",
        "    torch.randn(timestamps) * 0.1 - 10.0,\n",
        "    torch.randn(timestamps) + 0.5,\n",
        "])\n",
        "mean = time_series.mean()\n",
        "\n",
        "\n",
        "beta = 0.9\n",
        "ema_star = 0.0\n",
        "history, history_star = [], []\n",
        "\n",
        "for t, d in enumerate(time_series):\n",
        "\n",
        "    # This is the ema update\n",
        "    ema, ema_star = ema_update(d, beta, t+1, ema_star)\n",
        "    history.append(ema)\n",
        "    history_star.append(ema_star)\n",
        "\n",
        "plt.title('Exponential Moving Average Test Data')\n",
        "plt.scatter(range(len(time_series)), time_series, s=1, color='red', label='observed data')\n",
        "plt.axhline(mean, label='ordinary mean', color='magenta')\n",
        "plt.plot(history, label='EMA')\n",
        "plt.plot(history_star, label='EMA*', linewidth=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9f207c",
      "metadata": {
        "id": "7d9f207c"
      },
      "source": [
        "<font size='4'>**III) ADAM Optimizer**</font>\n",
        "\n",
        "ADAM (Adaptive Moment Estimation) is a popular optimization algorithm used for training machine learning and deep learning models.  ADAM is known for its efficiency, robustness, and ability to handle a wide range of optimization problems.\n",
        "\n",
        "Here are the key components and features of the ADAM optimizer:\n",
        "\n",
        "- Adaptive Learning Rates: ADAM adapts the learning rates for each parameter during training. It maintains a separate learning rate for each parameter based on the historical gradients of that parameter. This adaptability helps the algorithm converge faster and handle sparse gradients effectively.\n",
        "\n",
        "- EMA Momentum: ADAM incorporates the concept of momentum, similar to the SGD with momentum optimizer. It uses exponential moving averages of past gradients to help the optimization process. This momentum term smooths the optimization trajectory and accelerates convergence.\n",
        "\n",
        "The update rule for ADAM is as follows:\n",
        "\n",
        "\\begin{align*}\n",
        "m_t^* & = \\beta_1 \\cdot m_{t-1}^* + (1 - \\beta_1) \\cdot g_t & & \\text{(First Moment)} \\\\\n",
        "m_t & = \\frac{m_t^*}{1 - \\beta_1^t} & &\\text{(Bias correction)} \\\\\n",
        "v_t^* & = \\beta_2 \\cdot v_{t-1}^* + (1 - \\beta_2) \\cdot g_t^2 & & \\text{(Second Moment)} \\\\\n",
        "v_t & = \\frac{v_t^*}{1 - \\beta_2^t} & & \\text{(Bias correction)} \\\\\n",
        "x_{t+1} & = x_t - \\alpha \\cdot \\frac{{m}_t}{\\sqrt{{v}_t} + \\epsilon} & &\\text{(Parameter Update)}\n",
        "\\end{align*}\n",
        "\n",
        "Where:\n",
        "\n",
        "- $m_t$ and $v_t$ - are the EMA estimates of the first and second moments of the gradients at time step t, respectively.\n",
        "- $\\beta_1$ and $\\beta_2$ - are exponential decay rates for the first and second moments, typically close to 1\n",
        "- $g_t$ - is the gradient of the parameter $\\theta$ at time step t, which is used to calculate these moving averages.\n",
        "- $\\alpha$ is the learning rate.\n",
        "- $\\epsilon$ is a small constant, just to avoid division-by-zero.\n",
        "\n",
        "This can also be written as\n",
        "\\begin{align*}\n",
        "m_t, m_t^* & = \\mathtt{ema\\_update}(g_t, \\beta_1, t, m_{t-1}^*) & &\\text{(EMA Update)}\\\\\n",
        "v_t, v_t^* & = \\mathtt{ema\\_update}(g_t^2, \\beta_1, t, v_{t-1}^*) & &\\text{(EMA Update)} \\\\\n",
        "x_{t+1} & = x_t - \\alpha \\cdot \\frac{{m}_t}{\\sqrt{{v}_t} + \\epsilon} & &\\text{(Parameter Update)}\n",
        "\\end{align*}\n",
        "\n",
        "As a final detail, we can incorporate weight decay by defining $g_t$ to include the weight decay term as worked out in a previous exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wyuRowA8r_jK",
      "metadata": {
        "id": "wyuRowA8r_jK"
      },
      "source": [
        "<font size='4' color='Red'>Task 2.3 - Implement the ADAM optimizer (_ points)</font>\n",
        "\n",
        "Now implement your own ADAMOptimizer, using the following code as a starting point.\n",
        "\n",
        " * Within the loop, you can use `ema_update` to make the code simpler.\n",
        " * Remember that there are two smoothing factors, `beta_1` and `beta_2`.\n",
        " * Remember to incorporate hyperparameters `learning rate`, `epsilon`, and `weight_decay`\n",
        "\n",
        "You can test your code by swapping between `torch.optim.Adam` and your own `ADAMOptimizer`.  On this test, they should behave the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4f52ff",
      "metadata": {
        "id": "cf4f52ff"
      },
      "outputs": [],
      "source": [
        "class ADAMOptimizer():\n",
        "    def __init__(self, parameters, lr=0.1, betas=[0.9, 0.999], eps=1e-8, weight_decay=0.0):\n",
        "        self.lr = lr\n",
        "        self.beta_1, self.beta_2 = betas\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        # t is a running timestamp.\n",
        "        self.t = 0\n",
        "        self.parameters = []\n",
        "        self.m_star = []\n",
        "        self.v_star = []\n",
        "        for x in parameters:\n",
        "            self.parameters.append(x)\n",
        "            self.m_star.append(torch.zeros_like(x))\n",
        "            self.v_star.append(torch.zeros_like(x))\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        with torch.no_grad():\n",
        "            for x, m_star, v_star in zip(self.parameters, self.m_star, self.v_star):\n",
        "                g = 0.0\n",
        "                m, m_star[...] = 0, m_star\n",
        "                v, v_star[...] = 0, v_star\n",
        "                ################################################################\n",
        "                # TODO: Implement ADAM optimization\n",
        "                #\n",
        "                # There are three steps:\n",
        "                # (1) m and m_star are updated to incorporate g with smoothing\n",
        "                # beta_1 using ema_update(...).\n",
        "                # (2) v and v_star are updated to incorporate g^2 with smoothing\n",
        "                # beta_2 using ema_update(...).\n",
        "                #(3) x is updated according to ratio between the mean and RMS\n",
        "                # gradient estimates, using lr and epsilon.\n",
        "                #\n",
        "                # Remember to advance the timestep t before each step.\n",
        "                # Remember to update m_star and v_star in-place.\n",
        "                # There is a difference between saying m_star = something and\n",
        "                # m_star[...] = something.\n",
        "                ################################################################\n",
        "\n",
        "                x = None\n",
        "\n",
        "                ################################################################\n",
        "                #                             END OF YOUR CODE                 #\n",
        "                ################################################################\n",
        "    def zero_grad(self):\n",
        "        for x in self.parameters:\n",
        "            x.grad = None\n",
        "\n",
        "\n",
        "# TEST CODE BELOW\n",
        "# 'x' variable = is the current estimate which will be updated iteratively during the optimization.\n",
        "# 'L' function = the quadratic loss function we will use.  This one can also plot its inputs and outputs.\n",
        "# 'learning_rate' = is the step size that is used to update the solution\n",
        "\n",
        "x = torch.tensor([-1.5, 1.2])\n",
        "x.requires_grad = True # This tells PyTorch that the x variable will be used to calculate gradients.\n",
        "\n",
        "L = LossFunctionWithPlot()\n",
        "\n",
        "learning_rate = 0.1\n",
        "weight_decay = 0.1\n",
        "\n",
        "# If you switch this for torch.optim.Adam, it should behave exactly the same.\n",
        "optimizer = ADAMOptimizer([x], lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for iter in range(50):\n",
        "    loss = L(x)\n",
        "    if iter % 7 == 0: print(f'Loss at step {iter} is {loss.item():.3f}')\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "L.plot_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LY2CIwLNAb9j",
      "metadata": {
        "id": "LY2CIwLNAb9j"
      },
      "source": [
        "##<font size='5'>**Part 3: Training a Neural Network**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VFusvmFrC86H",
      "metadata": {
        "id": "VFusvmFrC86H"
      },
      "source": [
        "###<font size='4'>**Tiny Classification Problem**</font>\n",
        "\n",
        "The following code loads raw data for a tiny classification problem.\n",
        "\n",
        "\n",
        "The training data has 10000 samples, each a vector of 36 numbers along with\n",
        "a corresponding set of 10000 labels, assigning 0 or 1 to each sample.\n",
        "The test data has 2000 samples and labels that are disjoint from the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96435708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96435708",
        "outputId": "c26cbdf7-1c2f-483f-a708-4630d443e703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training data has 8000 samples, each a vector of 36 numbers along with\n",
            "a corresponding set of 8000 labels, assigning 0.0 or 1.0 to each sample.\n",
            "The test data has 1000 samples and labels that are disjoint from the training data.\n"
          ]
        }
      ],
      "source": [
        "train_data, train_labels, test_data, test_labels = [\n",
        "    torch.tensor(m[k]).float()\n",
        "    for m in [numpy.load('tiny-classification.npz')]\n",
        "    for k in 'train_data train_labels val_data val_labels'.split()]\n",
        "\n",
        "print(f'The training data has {train_data.size(0)} samples, each a vector of {train_data.size(1)} numbers along with')\n",
        "print(f'a corresponding set of {train_labels.size(0)} labels, assigning {train_labels.min()} or {train_labels.max()} to each sample.')\n",
        "\n",
        "print(f'The test data has {test_data.size(0)} samples and labels that are disjoint from the training data.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "epMu2hf6gGcK",
      "metadata": {
        "id": "epMu2hf6gGcK"
      },
      "source": [
        "\n",
        "\n",
        "The following code cell serves the purpose of training a neural network, tracking its performance throughout the training process, and generating visual representations of the training progress. It is crucial for students to grasp the functionality of the `run_test` function and `Supervise` class as it will be frequently utilized in the subsequent tasks.\n",
        "\n",
        "Comments have been thoughtfully included to enhance code comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L1S1BFVngGm9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1S1BFVngGm9",
        "outputId": "cb3bba7c-6b8a-4e20-8f58-0c3495075237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data width 36; Constant baseline accuracy 0.500\n"
          ]
        }
      ],
      "source": [
        "def run_test(net, optmaker, test_every=10):\n",
        "    # Set up the Loss Function and Optimizer\n",
        "    optimizer = optmaker(net.parameters()) # Initialize the optimizer with model parameters\n",
        "    print(f'{sum([p.numel() for p in net.parameters()])} parameters')\n",
        "    train_losses, train_accs, test_accs = [], [], []\n",
        "\n",
        "    for epoch in range(2000):\n",
        "        loss = net(train_data.float(), train_labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append([epoch, loss.item()])\n",
        "        optimizer.step() # Update model parameters using the optimizer's update rule\n",
        "        if epoch % test_every == test_every - 1:\n",
        "            grads = torch.stack([p.grad.abs().max() for p in net.parameters()])\n",
        "            maxg, ming = grads.abs().max(), grads.abs().min()\n",
        "            train_outputs = net.net(train_data.float())\n",
        "            train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "            train_accuracy = (train_preds == train_labels).float().mean()\n",
        "            train_accs.append([epoch + 1, train_accuracy])\n",
        "            test_outputs = net.net(test_data.float())\n",
        "            test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "            test_accuracy = (test_preds == test_labels).float().mean()\n",
        "            test_accs.append([epoch + 1, test_accuracy])\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item():.5f}, Grad range {maxg:.1e} to {ming:.1e}, '\n",
        "                  f'Train Accuracy: {train_accuracy.item()}, Test Accuracy: {test_accuracy.item()}', end='   \\r')\n",
        "            if test_accuracy.item() == 1.0:\n",
        "                break\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Test the Model\n",
        "    with torch.no_grad():\n",
        "        train_outputs = net.net(train_data.float())\n",
        "        train_preds = (train_outputs.squeeze() > 0.5).float()\n",
        "        train_accuracy = (train_preds == train_labels).float().mean()\n",
        "        test_outputs = net.net(test_data.float())\n",
        "        test_preds = (test_outputs.squeeze() > 0.5).float()\n",
        "        test_accuracy = (test_preds == test_labels).float().mean()\n",
        "        print(f'\\nTrain Accuracy: {train_accuracy.item():.5f}, Test Accuracy: {test_accuracy.item():.5f}')\n",
        "\n",
        "\n",
        "    # Visulaiztion\n",
        "    fig, ax = plt.subplots()\n",
        "    ax2 = ax.twinx()\n",
        "    ax.plot(*zip(*train_losses), label=\"Training loss\")\n",
        "    ax.set_yscale('log')\n",
        "    ax2.plot(*zip(*train_accs), color=\"orange\", label=\"Training accuracy\")\n",
        "    ax2.plot(*zip(*test_accs), color=\"red\", label=\"Test accuracy\")\n",
        "    ax2.set_ylim(0.0, 1.0)\n",
        "    for a in [ax, ax2]:\n",
        "        for pos in 'top right bottom left'.split():\n",
        "            a.spines[pos].set_visible(False)\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    fig.legend(loc=\"lower left\", bbox_to_anchor=(0, 0), bbox_transform=ax.transAxes)\n",
        "    fig.show()\n",
        "\n",
        "print(f'Data width {train_data.size(1)}; Constant baseline accuracy {max(test_labels.sum(), len(test_labels) - test_labels.sum()) / len(test_labels):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y5qSRWwijUQa",
      "metadata": {
        "id": "Y5qSRWwijUQa"
      },
      "source": [
        "The `Supervise` class is a wrapper that combines a neural network model and a loss function to facilitate supervised learning tasks. It computes the loss by performing a forward pass through the neural network and comparing the predicted values to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VlilWz24jHZ6",
      "metadata": {
        "id": "VlilWz24jHZ6"
      },
      "outputs": [],
      "source": [
        "class Supervise(Module):\n",
        "    def __init__(self, criterion, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "    def forward(self, x, y):\n",
        "        out = self.net(x).squeeze()\n",
        "        return self.criterion(out, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6NtCZCwld6dL",
      "metadata": {
        "id": "6NtCZCwld6dL"
      },
      "source": [
        "<font size='4'>**Here is an example on how to train a model using `Supervise` and `run_test` function**</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO: Build a Neural Network which has the architecture as follows:-\n",
        "# Hidden Dimension - 256\n",
        "# Loss function - Mean Squared Error (MSE)\n",
        "# Optimizer - Simple GradientDescent (lr=0.1) [Using the optimizer built in Task 1.3]\n",
        "# Network Architecture - (Linear + Sigmoid) -> (Linear + Sigmoid) -> (Linear +\n",
        "# Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "input_size=train_data.size(1)\n",
        "hidden_dims= 256\n",
        "output_dims=1\n",
        "\n",
        "run_test(\n",
        "    Supervise(\n",
        "    nn.MSELoss(),\n",
        "        nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_dims),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(hidden_dims, hidden_dims),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(hidden_dims, output_dims),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    ),\n",
        "    lambda p: SimpleGradientDescent(p,lr=0.1)\n",
        ")\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "fZBTDTHdf7oT",
        "outputId": "59ede355-36ae-42f9-b518-435b5728da6d"
      },
      "id": "fZBTDTHdf7oT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75521 parameters\n",
            "Epoch 2000, Loss: 0.24976, Grad range 1.5e-04 to 1.6e-06, Train Accuracy: 0.5406249761581421, Test Accuracy: 0.5339999794960022   \n",
            "Train Accuracy: 0.54062, Test Accuracy: 0.53400\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64K8wl6wmrwT",
      "metadata": {
        "id": "64K8wl6wmrwT"
      },
      "source": [
        "<font size='4'>**Configuring and training various Neural Network Architectures**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EQyIGjm0-xTP",
      "metadata": {
        "id": "EQyIGjm0-xTP"
      },
      "source": [
        "Let's construct a range of neural network architectures with varying configurations to explore their impact on training performance when considering factors such as:\n",
        "\n",
        "- Activation functions\n",
        "- Choice of optimizers\n",
        "- Regulariztion\n",
        "- Hidden layer dimensions\n",
        "- Network depth\n",
        "- Batch normalization\n",
        "- Residual networks.\n",
        "\n",
        "This experimentation will help us gain insights into how these factors influence the training process on our dataset.\n",
        "\n",
        "**Note** - Utilize the previously defined `SimpleGradientDescent` and `ADAMOptimizer` optimizers exclusively, unless otherwise specified in the task to employ PyTorch's built-in optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LiuIvhPU3YMH",
      "metadata": {
        "id": "LiuIvhPU3YMH"
      },
      "source": [
        "#### **1) Activation functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agozVVyorrp5",
      "metadata": {
        "id": "agozVVyorrp5"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.1 - Test sigmoid activations in a three-layer network with `run_Test` (1 point)</font>\n",
        "\n",
        "Let's apply our new `run_test` function together with your `SimpleGradientDescent` optimizer class to test (assuming that is the class name you gave it).  You will need to put in your three-layer sigmoid network definition to make the example work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d183dbc",
      "metadata": {
        "id": "2d183dbc"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Create an architecture similar to the example, but an extra layer\n",
        "# is added to this architecture\n",
        "# Hidden Dimension - 128\n",
        "# Loss - MSELoss()\n",
        "# Optimizer - Simple GradientDescent - (lr = 1.0) [Use the Optimizer built above]\n",
        "# Network Architecture - (Linear + Tanh) -> (Linear + Tanh) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "raise NotImplementedError\n",
        "\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aWO4UYrloOdv",
      "metadata": {
        "id": "aWO4UYrloOdv"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.2 - Test Tanh activations in a three-layer network (1 point)</font>\n",
        "\n",
        "Implement the same neural network architecture\n",
        "\n",
        "Read documentation - https://pytorch.org/docs/stable/generated/torch.nn.Tanh.AC\n",
        "\n",
        "Now let us compare Sigmoid to other activations.  Switch every `nn.Sigmoid()` to `nn.Tanh()`.  Tanh balances positive and negative outputs and sometimes work better.  Implement and decide whether it is helping in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6lgkGpy0pLfs",
      "metadata": {
        "id": "6lgkGpy0pLfs"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Create an architecture similar to the previous one, but replace every\n",
        "# nn.Sigmoid() activation function with nn.Tanh() (except for the last one).\n",
        "# Hidden Dimension - 128\n",
        "# Loss - MSELoss()\n",
        "# Optimizer - Simple GradientDescent - (lr = 1.0) [Use the Optimizer built above]\n",
        "# Network Architecture - (Linear + Tanh) -> (Linear + Tanh) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kyb8euJ4rVv_",
      "metadata": {
        "id": "Kyb8euJ4rVv_"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.3 - Test ReLU activations in a three-layer network (1 point)</font>\n",
        "\n",
        "The ReLU activation was studied closely by Glorot, which we have discussed in class.  It tends to be very effective at avoiding  vanishing gradients, because on the positive side it never saturates.  Replace all your nonlinearties with ReLU while keeping the architecture otherwise the same.  Does ReLU help in this case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqP39Nbi3q5-",
      "metadata": {
        "id": "BqP39Nbi3q5-"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "#############################################################################################################\n",
        "# TODO: Create an architecture similar to the previous one, but replace every\n",
        "# nn.Tanh() activation function with nn.ReLU() (except for the last one).\n",
        "# Hidden Dimension - 128\n",
        "# Loss - MSELoss()\n",
        "# Optimizer - SimpleGradientDescent - (lr = 1.0)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "#############################################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uhK2MS4psHKf",
      "metadata": {
        "id": "uhK2MS4psHKf"
      },
      "source": [
        "<font size='4' color='red'>**3.A) Inline Question (1 point):**  \n",
        "In the above Tasks, we employed three different activation functions—sigmoid, tanh, and relu—in our neural network architecture. Describe the difference in the behavior of the optimization process that you observed between Sigmoid, TanH, and ReLU.  Do your results confirm or contradict the results that Glorot reported in his 2010 study?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uW4GDdwfhDeW",
      "metadata": {
        "id": "uW4GDdwfhDeW"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_1AkeUKK4foW",
      "metadata": {
        "id": "_1AkeUKK4foW"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.4 - Implement Binary Cross Entropy Loss (1 point)</font>\n",
        "\n",
        "In a classification setting, we often prefer to interpret the outputs as probabilities and drive the probability distribution towards the true distribution.  The standard way to achieve that is to use the cross-entropy loss.  Cross-entropy (as seen in HW1) also test to avoid saturation when compared to MSE, when used in combination with softmax.\n",
        "\n",
        "Replace the supervision with `BCELoss` rather than mean square error, and observe any differences.\n",
        "\n",
        "Read documentation for BCE Loss - https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B_R-exYg43AB",
      "metadata": {
        "id": "B_R-exYg43AB"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Build a neural network with following architecture:\n",
        "# Hidden Dimension - 128\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - SimpleGradientDescent - (lr = 1.0)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "khIGKQdaxIaY",
      "metadata": {
        "id": "khIGKQdaxIaY"
      },
      "source": [
        "####<font size='4'>**2) Optimizers**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O10S7rvSx2tT",
      "metadata": {
        "id": "O10S7rvSx2tT"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.5 Implement using SGD Optimizer (1 point)</font>\n",
        "\n",
        "So far we have been using our own `SimpleGradientDescent`.  Now try comparing results with pytorch's built-in `torch.optim.SGD` class.  How does your implementation compare?  Is it the same?\n",
        "\n",
        "Read Documentation for SGD Optimizer using Pytorch - https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GX8ozUIJxHgo",
      "metadata": {
        "id": "GX8ozUIJxHgo"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Build a neural network with following architecture, Careful, Hidden\n",
        "# Dimension has changed.\n",
        "# Hidden Dimension - 128\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - SGD - (lr = 0.5)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0UWdkIJ26CuR",
      "metadata": {
        "id": "0UWdkIJ26CuR"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.6 - ADAM Optimizer (1 point)</font>\n",
        "\n",
        "ADAM is a very powerful optimizer and should improve results.\n",
        "\n",
        "Use your own `ADAMOptimizer` class here to see how it behaves.  If you wish to debug against the standard ADAM optimizer, then you can try it out as well, but when you hand in your results, show what your `ADAMOptimzer` does.  Ideally, they should behave the same.\n",
        "\n",
        "Read Documentation for ADAM Optimizer using Pytorch - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-mhg9qJz6JVz",
      "metadata": {
        "id": "-mhg9qJz6JVz"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Build a neural network with following architecture:\n",
        "# Hidden Dimension - 128\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - ADAM (weight decay = 0 ,lr = 0.01)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHYOLEdrpxds",
      "metadata": {
        "id": "fHYOLEdrpxds"
      },
      "source": [
        "<font size='4' color='red'>**3.B) Inline Question (2 points):** Does ADAMOptimzer do better?  Explain how the optimization behaves differently than you observed with Simple Gradient Descent. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qrX35UIftVrp",
      "metadata": {
        "id": "qrX35UIftVrp"
      },
      "source": [
        "Ans."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Bn8oC7m7tBo",
      "metadata": {
        "id": "8Bn8oC7m7tBo"
      },
      "source": [
        "<font size='4'>**The Battle Against Overfitting**</font>\n",
        "\n",
        "Let's apply some techniques to enhance and refine our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7gvyvcLJz6sZ",
      "metadata": {
        "id": "7gvyvcLJz6sZ"
      },
      "source": [
        "####<font size='4'>**A) The Art of Regularization**</font>\n",
        "\n",
        "\n",
        "<font size='4' color='Red'>Task 3.7 - Add a penalty term to the loss function that discourages large weights. This helps prevent the model from fitting noise in the data. (1 point)</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-LXnpOum6zqQ",
      "metadata": {
        "id": "-LXnpOum6zqQ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150) # (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Design a neural network with the specified architecture and introduce a\n",
        "# regularization term in the loss function to discourage the growth of large\n",
        "# weights.\n",
        "# Hidden Dimension - 128\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam with weight decay - (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mh2MW5Kg2H45",
      "metadata": {
        "id": "Mh2MW5Kg2H45"
      },
      "source": [
        "####<font size='4'>**B) Slimming Down for Success**</font>\n",
        "\n",
        "Reducing Hidden Dimension - Fewer parameters means that the model has less flexibility to fit the training data and it is forced to learn simpler features that are more likely to generalize to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73PD2qLp8E-J",
      "metadata": {
        "id": "73PD2qLp8E-J"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.8 - Going Deeper with Shrinking Hidden Dimensions (1 point)</font>\n",
        "\n",
        "Let's explore the effectiveness of dimension reduction as a technique by reducing the number of hidden dimensions from 128 to 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TFzA3PE38k_4",
      "metadata": {
        "id": "TFzA3PE38k_4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 64\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam with weight decay - (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IaB_IxhL8zpR",
      "metadata": {
        "id": "IaB_IxhL8zpR"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.10 - Going more Deeper with Shrinking Hidden Dimensions (1 point)</font>\n",
        "\n",
        "Dimension reduction improved our outcome. Let's continue by further reducing the hidden dimension from 64 to 32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14060344",
      "metadata": {
        "id": "14060344"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam with weight decay - (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X9mgkLxp9O_4",
      "metadata": {
        "id": "X9mgkLxp9O_4"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.11 - The Last Shrink (1 point)</font>\n",
        "\n",
        "Did it help to reduce to 32 dimensions?\n",
        "\n",
        "Let's try one more time, maybe it will work? (Below, please try reducing the hidden dimension from 32 to 16.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ckr9PX6-9YHo",
      "metadata": {
        "id": "Ckr9PX6-9YHo"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "#############################################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 16\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam with weight decay - (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "#############################################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9QABLUcI_H2s",
      "metadata": {
        "id": "9QABLUcI_H2s"
      },
      "source": [
        "<font size='4' color='red'>**3.C) Inline Question**  - In the above Tasks [3.8 - 3.11] (Slimming Down for success) what were your key observations? (1 point)</font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2A7yApTVQz1D",
      "metadata": {
        "id": "2A7yApTVQz1D"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M_PqcsLi9fkA",
      "metadata": {
        "id": "M_PqcsLi9fkA"
      },
      "source": [
        "####<font size='4'>**C) Layer by Layer**</font>\n",
        "\n",
        "Increasing the number of layers.\n",
        "\n",
        "Geoff Hinton likes to assert that deeper layers can capture increasingly abstract and high-level features in the data. This hierarchy allows the network to focus on relevant patterns and discard noise, making it less prone to fitting random variations in the training data.\n",
        "\n",
        "Is it true?  Let's try it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8x2-KnATDNwo",
      "metadata": {
        "id": "8x2-KnATDNwo"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.12 - Increasing the Depth of the network (1 point)</font>\n",
        "\n",
        "Let's enhance our model by introducing an additional layer, creating a network with four layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52857f7e",
      "metadata": {
        "id": "52857f7e"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 64\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + ReLU)\n",
        "# -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9W2sf6W3-xBR",
      "metadata": {
        "id": "9W2sf6W3-xBR"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.13 - Increasing the Depth and reducing the width of the network (1 point)</font>\n",
        "\n",
        "The recent modification yielded remarkable results. Now, let's take it a step further by enhancing our model's architecture: we'll increase the number of layers from 4 to 6 and reduce the hidden dimension from 64 to 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iKXs0g9w_Cx4",
      "metadata": {
        "id": "iKXs0g9w_Cx4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam with weight decay - (lr=0.005, weight_decay=1e-4)\n",
        "# Network Architecture - (Linear + ReLU) -> (Linear + ReLU) -> (Linear + ReLU)\n",
        "#-> (Linear + ReLU) -> (Linear + ReLU) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TzOGOppCXECH",
      "metadata": {
        "id": "TzOGOppCXECH"
      },
      "source": [
        "####<font size='4'>**D) Batch Normalization**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-QyHKtasenOh",
      "metadata": {
        "id": "-QyHKtasenOh"
      },
      "source": [
        "Batch Normalization is a technique used to improve the training of deep neural networks. It works by normalizing the activations of each layer, which helps to prevent the network from becoming too sensitive to the initialization of the weights and the order of the training data.\"\n",
        "\n",
        "Pytorch documentation - https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pv1ChQAeUBoQ",
      "metadata": {
        "id": "pv1ChQAeUBoQ"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.14 Adding Batch Normalization (1 point)</font>\n",
        "\n",
        "In the previous tasks, our model with a hidden dimension of 32 didn't deliver the desired performance. To address this, let's incorporate Batch Normalization into that architecture and assess whether it can enhance its performance.\n",
        "\n",
        "Read documentation - https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6sx33jmSXJ-o",
      "metadata": {
        "id": "6sx33jmSXJ-o"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: Let's create a neural network with following architecture:\n",
        "# Hidden Dimension - 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam (lr = 0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU + batch_normalization) ->\n",
        "# (Linear + ReLU) -> (Linear + ReLU) -> (Linear + ReLU) ->\n",
        "# (Linear + ReLU + batch_normalization) -> (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jy_YOZm1YAbw",
      "metadata": {
        "id": "Jy_YOZm1YAbw"
      },
      "source": [
        "####<font size='4'>**E) Residual Networks**</font>\n",
        "\n",
        "Residual Networks (ResNets) are a type of deep neural network that are designed to address the problem of vanishing gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hGQveZgVU2K9",
      "metadata": {
        "id": "hGQveZgVU2K9"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.15 - Modifiy the code below so that that `ResidualSequence` does not just implement `y = f(x)` but instead implements `y = f(x) + x`.  The template code has a bug and only implement `y=x`. (1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VqeqI1MBBlUJ",
      "metadata": {
        "id": "VqeqI1MBBlUJ"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# TODO: Correct the below code to get f(x) + x as output\n",
        "################################################################################\n",
        "class ResidualSequence(Sequential):\n",
        "    def forward(self, x):\n",
        "        side_result = super().forward(x)\n",
        "        final_result=x\n",
        "        return final_result\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aoffvEEYVdD2",
      "metadata": {
        "id": "aoffvEEYVdD2"
      },
      "source": [
        "<font size='4'>**Example of Residual Block Architecture**</font>\n",
        "\n",
        "```\n",
        "torch.nn.Sequential(\n",
        "    nn.Linear(train_data.size(1), hidden_dims),\n",
        "    ResidualSequence(\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dims, fan_out_dims),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fan_out_dims, hidden_dims),\n",
        "    ),\n",
        "    nn.Linear(hidden_dims, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YvdYH6DgTcHt",
      "metadata": {
        "id": "YvdYH6DgTcHt"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.16 - Implement Residual blocks in a Neural Network architecture (2 point)</font>\n",
        "\n",
        "Design a neural network with four Residual blocks, each composed according to the specifications outlined below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f39252",
      "metadata": {
        "id": "b9f39252"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "#############################################################################\n",
        "# TODO: In this task, we will configure a neural network consisting of four\n",
        "# residual blocks\n",
        "# Hidden Dimension = 16\n",
        "# fan_out_dims = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam- (lr=0.01, weight_decay=1e-3)\n",
        "# Residual Block - [ReLU → Linear → ReLU → Linear]\n",
        "# Network Architecture - Linear -> Residual Block -> Residual Block ->\n",
        "# Residual Block -> Residual Block -> (Linear + Sigmoid)\n",
        "#############################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vqvQBJ70274f",
      "metadata": {
        "id": "vqvQBJ70274f"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.17 - Reducing Residual Blocks (2 points)</font>\n",
        "\n",
        "Let's decrease the number of Residual Blocks and observe whether it has any impact on our performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PoKUcI9u3ey4",
      "metadata": {
        "id": "PoKUcI9u3ey4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "hidden_dims = None\n",
        "fan_out_dims = None\n",
        "################################################################################\n",
        "# TODO: In this task, we will configure a neural network consisting of two\n",
        "# residual blocks\n",
        "# Hidden Dimension = 16\n",
        "# fan_out_dims = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam - (lr=0.01, weight_decay=1e-3)\n",
        "# Residual Block - [ReLU → Linear → ReLU → Linear]\n",
        "# Network Architecture - Linear -> Residual Block -> Residual Block ->\n",
        "# (Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kLZDNPS14DUF",
      "metadata": {
        "id": "kLZDNPS14DUF"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.18 Develop a Neural Network with a combination of BatchNorm and Residual Blocks (3 points)</font>\n",
        "\n",
        "The synergy of these two components often results in improved model performance because BatchNorm stabilizes activations and enables the use of deeper networks, while Residual connections facilitate the training of deep networks and prevent degradation in performance. Together, they can enhance the model's ability to learn intricate patterns and improve its generalization to unseen data. However, it's important to strike a balance and avoid overly complex models, as they may lead to overfitting if not properly regularized.\n",
        "\n",
        "We will create a neural network incorporating both Batch Normalization and Residual Blocks to evaluate if we can achieve favorable outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w-y9gsE_4NXZ",
      "metadata": {
        "id": "w-y9gsE_4NXZ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "\n",
        "#############################################################################\n",
        "# TODO: In this task, we will configure a neural network consisting of two\n",
        "# residual blocks,\n",
        "# Hidden Dimension - 16\n",
        "# fan_out_dims = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam - (lr = 0.01, weight_decay=1e-3)\n",
        "# Residual Block - [Batch_Norm -> Linear → ReLU → Linear]\n",
        "# Network Architecture - Linear -> Residual Block -> Residual Block ->\n",
        "# (Batch_Norm + Linear + Sigmoid)\n",
        "#############################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "###########################################################################\n",
        "#                             END OF YOUR CODE                            #\n",
        "###########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K36AJrhu5Tk4",
      "metadata": {
        "id": "K36AJrhu5Tk4"
      },
      "source": [
        "<font size='4' color='Red'>Task 3.19 - Develop a neural network that incorporates Batch Normalization, Residual connections, and an increased number of layers. (3 points)</font>\n",
        "\n",
        "We will construct a neural network with 6 residual blocks to assess whether we can further enhance performance by increasing its depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2XKznAk5iug",
      "metadata": {
        "id": "j2XKznAk5iug"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)# (Leave it here for deterministic behavior and easier grading)\n",
        "################################################################################\n",
        "# TODO: In this task, we will configure a neural network consisting of two\n",
        "# residual blocks.\n",
        "# Hidden Dimension - 16\n",
        "# fan_out_dims = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam - (lr = 0.01, weight_decay=1e-3)\n",
        "# Residual Block - [Batch_Norm -> Linear → ReLU → Linear]\n",
        "# Network Architecture - Linear -> Residual Block -> Residual Block ->\n",
        "# Residual Block -> Residual Block -> Residual Block -> Residual Block\n",
        "# -> (Batch_Norm + Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cRMMgZHRPYEV",
      "metadata": {
        "id": "cRMMgZHRPYEV"
      },
      "source": [
        "##<font size='5'>**Part 4: Weight Initialization**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462SzOwvPbpF",
      "metadata": {
        "id": "462SzOwvPbpF"
      },
      "source": [
        "Weight initialization in deep learning refers to the procedure of assigning initial values to a neural network's weights, which are the tunable parameters learned during training. The manner in which these weights are initialized plays a critical role in shaping the training process and ultimately impacts the network's performance. Several weight initialization techniques have been developed to combat challenges such as vanishing or exploding gradients, with the goal of establishing a solid foundation for training deep neural networks. Some of the weight initialization methods are :-\n",
        "\n",
        "1. Zero Weight Initialization\n",
        "2. Random Weight Initialization\n",
        "3. Xavier Initialization (Glorot Initialization)\n",
        "4. He Initialization (often used in deep CNN's)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vz_Pbf_W80y8",
      "metadata": {
        "id": "Vz_Pbf_W80y8"
      },
      "source": [
        "###<font size='4'>**1) Zero Initialization**</font>\n",
        "\n",
        "Zero weight initialization initializes a neural network's weights to zero, which can be effective for specific neural network types. This initialization can mitigate overfitting by making it harder for the model to fit training data perfectly.\n",
        "\n",
        "However, it has drawbacks. It hinders learning complex input-output relationships from scratch and makes the model sensitive to hyperparameters like the learning rate. This approach is generally discouraged due to the potential emergence of symmetric neurons and slow convergence as a result of weight symmetry issues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XbOL83LOAZ45",
      "metadata": {
        "id": "XbOL83LOAZ45"
      },
      "source": [
        "The code visualizes layer-wise activation distributions in a neural network for a Zero Weight Initialization. It offers insights into the network's learning and can reveal potential issues.\n",
        "\n",
        "<font size='4' color='Red'>Task 4.1 - Initialize zero weights for each layer (_ points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wlPj8u-x4uCY",
      "metadata": {
        "id": "wlPj8u-x4uCY"
      },
      "outputs": [],
      "source": [
        "num_layers = 7\n",
        "layer_dims = [2048]*num_layers\n",
        "\n",
        "input_data = np.random.randn(16, layer_dims[0])\n",
        "\n",
        "activations = []\n",
        "\n",
        "weights = []\n",
        "\n",
        "\n",
        "for i in range(num_layers - 1):\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO - # Initialize zero weights for each layer (except the last one)\n",
        "    # and store it in 'W' varaiable\n",
        "    ############################################################################\n",
        "    W = None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    weights.append(W)\n",
        "\n",
        "# Forward pass through the network\n",
        "for i in range(num_layers):\n",
        "    W = weights[i] if i < num_layers - 1 else None\n",
        "    layer_input = input_data if i == 0 else activations[i - 1]\n",
        "    if W is not None:\n",
        "        layer_output = np.tanh(np.dot(layer_input, W))\n",
        "    else:\n",
        "        layer_output = layer_input\n",
        "    activations.append(layer_output)\n",
        "\n",
        "# Create a figure with subplots for each layer's activation distribution\n",
        "fig, axes = plt.subplots(num_layers, 1, figsize=(8, 12))\n",
        "\n",
        "x_axis = np.arange(-1, 1, 0.01)\n",
        "\n",
        "for i in range(num_layers):\n",
        "    ax = axes[i]\n",
        "    ax.hist(activations[i].flatten(), bins=100, density=True, alpha=0.7, color='blue', label=f'Layer {i+1} Activations')\n",
        "    ax.plot(x_axis, norm.pdf(x_axis, 0, 1), color='red', linestyle='--', label='Standard Normal')\n",
        "    ax.set_title(f'Layer {i+1}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X7ds35hlCLWi",
      "metadata": {
        "id": "X7ds35hlCLWi"
      },
      "source": [
        "<font size='4' color='red'>**4.A) Inline Question (2 points):** Have you noticed any of the drawbacks in the results mentioned earlier? If so, please highlight your observations.</font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QzgqHyRfC0wF",
      "metadata": {
        "id": "QzgqHyRfC0wF"
      },
      "source": [
        "Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IIFVLjmyQGeN",
      "metadata": {
        "id": "IIFVLjmyQGeN"
      },
      "source": [
        "###<font size='4'>**2. Random Weight Initialization**</font>\n",
        "\n",
        "Random Weight Initialization in deep learning sets the initial weights of a neural network to random values, drawn from a distribution. It's simple and encourages diverse starting points for training, breaking symmetry among neurons. However, it can lead to vanishing/exploding gradients in deep networks, making it less effective for them. It's sensitive to initialization values and lacks control compared to specialized methods like Xavier or He initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4Uo270RR1e9",
      "metadata": {
        "id": "b4Uo270RR1e9"
      },
      "source": [
        "<font size='4' color='Red'>Task 4.2 Initialize Random weights for each layer (_ points)</font>\n",
        "\n",
        "The below code visualizes layer-wise activation distributions for a Random Weight Initialization in a neural network. It offers insights into the network's learning and can reveal potential issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eKGLAG0sOk_Y",
      "metadata": {
        "id": "eKGLAG0sOk_Y"
      },
      "outputs": [],
      "source": [
        "# Define the number of layers and their dimensions\n",
        "num_layers = 7\n",
        "layer_dims = [2048]*num_layers\n",
        "\n",
        "input_data = np.random.randn(16, layer_dims[0])\n",
        "\n",
        "activations = []\n",
        "\n",
        "weights = []\n",
        "\n",
        "\n",
        "for i in range(num_layers - 1):\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO - # Initialize random weights for each layer (except the last one)\n",
        "    # and the store value in 'W' variable\n",
        "    ############################################################################\n",
        "    W=None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    weights.append(W*0.01)\n",
        "\n",
        "# Forward pass through the network\n",
        "for i in range(num_layers):\n",
        "    W = weights[i] if i < num_layers - 1 else None\n",
        "    layer_input = input_data if i == 0 else activations[i - 1]\n",
        "    if W is not None:\n",
        "        layer_output = np.tanh(np.dot(layer_input, W))\n",
        "    else:\n",
        "        layer_output = layer_input\n",
        "    activations.append(layer_output)\n",
        "\n",
        "# Create a figure with subplots for each layer's activation distribution\n",
        "fig, axes = plt.subplots(num_layers, 1, figsize=(8, 12))\n",
        "\n",
        "x_axis = np.arange(-1, 1, 0.01)\n",
        "\n",
        "for i in range(num_layers):\n",
        "    ax = axes[i]\n",
        "    ax.hist(activations[i].flatten(), bins=100, density=True, alpha=0.7, color='blue', label=f'Layer {i+1} Activations')\n",
        "    ax.plot(x_axis, norm.pdf(x_axis, 0, 1), color='red', linestyle='--', label='Standard Normal')\n",
        "    ax.set_title(f'Layer {i+1}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dz8fgyYNE_XE",
      "metadata": {
        "id": "dz8fgyYNE_XE"
      },
      "source": [
        "<font size='4' color='red'>**4.B) Inline Question (2 points):** We can see all the activations tend to zero for deeper network layers. what can be expected regarding the gradients dL/dW, and is there still potential for learning?</font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "is_XFlZ4N9li",
      "metadata": {
        "id": "is_XFlZ4N9li"
      },
      "source": [
        "Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uXKaw5T6ULvu",
      "metadata": {
        "id": "uXKaw5T6ULvu"
      },
      "source": [
        "I can hear you all say try increasing weights which might resolve the issue. So, let's proceed by multiplying our weights by a factor of 5 and see if it helps.\n",
        "\n",
        "<font size='4' color='Red'>Task 4.3 - Initialize large Random weights for each layer (_ points)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QEha9BgXUQtk",
      "metadata": {
        "id": "QEha9BgXUQtk"
      },
      "outputs": [],
      "source": [
        "num_layers = 7\n",
        "layer_dims = [2048]*num_layers\n",
        "\n",
        "input_data = np.random.randn(16, layer_dims[0])\n",
        "\n",
        "activations = []\n",
        "\n",
        "weights = []\n",
        "\n",
        "\n",
        "for i in range(num_layers - 1):\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO - # Initialize random weights for each layer similar to the last\n",
        "    # task but here you scale them up by a factor of 5\n",
        "    ############################################################################\n",
        "    W = None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "    weights.append(W*0.01)\n",
        "\n",
        "# Forward pass through the network\n",
        "for i in range(num_layers):\n",
        "    W = weights[i] if i < num_layers - 1 else None\n",
        "    layer_input = input_data if i == 0 else activations[i - 1]\n",
        "    if W is not None:\n",
        "        layer_output = np.tanh(np.dot(layer_input, W))\n",
        "    else:\n",
        "        layer_output = layer_input\n",
        "    activations.append(layer_output)\n",
        "\n",
        "# Create a figure with subplots for each layer's activation distribution\n",
        "fig, axes = plt.subplots(num_layers, 1, figsize=(8, 12))\n",
        "\n",
        "x_axis = np.arange(-1, 1, 0.01)\n",
        "\n",
        "for i in range(num_layers):\n",
        "    ax = axes[i]\n",
        "    ax.hist(activations[i].flatten(), bins=100, density=True, alpha=0.7, color='blue', label=f'Layer {i+1} Activations')\n",
        "    ax.plot(x_axis, norm.pdf(x_axis, 0, 1), color='red', linestyle='--', label='Standard Normal')\n",
        "    ax.set_title(f'Layer {i+1}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yGsWlxwXp-Kd",
      "metadata": {
        "id": "yGsWlxwXp-Kd"
      },
      "source": [
        "<font size='4' color='red'>**4.C) Inline Question (2 points):**All activations saturate due to big weights. What can be expected regarding the gradients dL/dW, and is there still potential for learning?</font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l0JMbmK6R4wO",
      "metadata": {
        "id": "l0JMbmK6R4wO"
      },
      "source": [
        "Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "woHhwkC9Uatl",
      "metadata": {
        "id": "woHhwkC9Uatl"
      },
      "source": [
        "###<font size='4'>**3. Xavier Initialization**</font>\n",
        "\n",
        "Xavier Initialization, also called Glorot Initialization, is a weight initialization method for deep neural networks. It sets initial weights to prevent vanishing and exploding gradients by controlling the variance of activations.This technique stabilizes training and is widely used in practice.\n",
        "\n",
        "The weights are initialized from a Gaussian distribution with a mean of 0 and a variance of (1/$n_{in}$) :-\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{W} = \\mathcal{N}\\left(0, \\frac{1}{n_\\text{in}}\\right)\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-tEtJNVb9I8",
      "metadata": {
        "id": "k-tEtJNVb9I8"
      },
      "source": [
        "The below code visualizes layer-wise activation distributions for a Xavier Weight Initialization in a neural network. It offers insights into the network's learning and can reveal potential issues.\n",
        "\n",
        "\n",
        "<font size='4' color='Red'>Task 4.4 - Initialize weights using Xavier method for each layer (1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45S4hcaSUzat",
      "metadata": {
        "id": "45S4hcaSUzat"
      },
      "outputs": [],
      "source": [
        "num_layers = 7\n",
        "layer_dims = [2048]*num_layers\n",
        "\n",
        "input_data = np.random.randn(16, layer_dims[0])\n",
        "activations = []\n",
        "weights = []\n",
        "\n",
        "\n",
        "for i in range(num_layers - 1):\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO - Initialize weights using Xavier method for each layer (except the\n",
        "    # last one) and store in variable 'W'\n",
        "    ############################################################################\n",
        "    W= None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    weights.append(W)\n",
        "\n",
        "# Forward pass through the network\n",
        "for i in range(num_layers):\n",
        "    W = weights[i] if i < num_layers - 1 else None\n",
        "    layer_input = input_data if i == 0 else activations[i - 1]\n",
        "    if W is not None:\n",
        "        layer_output = np.tanh(np.dot(layer_input, W))\n",
        "    else:\n",
        "        layer_output = layer_input\n",
        "    activations.append(layer_output)\n",
        "\n",
        "# Create a figure with subplots for each layer's activation distribution\n",
        "fig, axes = plt.subplots(num_layers, 1, figsize=(8, 12))\n",
        "\n",
        "x_axis = np.arange(-1, 1, 0.01)\n",
        "\n",
        "for i in range(num_layers):\n",
        "    ax = axes[i]\n",
        "    ax.hist(activations[i].flatten(), bins=100, density=True, alpha=0.7, color='blue', label=f'Layer {i+1} Activations')\n",
        "    ax.plot(x_axis, norm.pdf(x_axis, 0, 1), color='red', linestyle='--', label='Standard Normal')\n",
        "    ax.set_title(f'Layer {i+1}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S_I77IN1ZD_f",
      "metadata": {
        "id": "S_I77IN1ZD_f"
      },
      "source": [
        "###<font size='4'>**4. He/ MSRA Initialization**</font>\n",
        "\n",
        "It is a weight initialization technique commonly used in deep neural networks. It is designed to address the vanishing gradient problem and is particularly effective when Rectified Linear Unit (ReLU) activation functions are used.\n",
        "\n",
        "For a layer with $n_{in}$ input units, He Initialization initializes the weights by sampling them from a Gaussian distribution with a mean of 0 and a variance of 2 / $n_{in}$. The choice of variance (2) is specific to the ReLU activation function and ensures that the weights are set to values that allow ReLU units to activate in a desirable range.\n",
        "\n",
        "The formula for He Initialization can be expressed as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{W} = \\mathcal{N}\\left(0, \\frac{2}{n_\\text{in}}\\right)\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pmvprA2JiQGF",
      "metadata": {
        "id": "pmvprA2JiQGF"
      },
      "source": [
        "The below code visualizes layer-wise activation distributions for a HE/ MSRA Weight Initialization in a neural network. It offers insights into the network's learning and can reveal potential issues.\n",
        "\n",
        "\n",
        "<font size='4' color='Red'>Task 4.5- Initialize weights using Kaiming He's method for each layer (1 point)</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bCEuRjU-ZImD",
      "metadata": {
        "id": "bCEuRjU-ZImD"
      },
      "outputs": [],
      "source": [
        "num_layers = 7\n",
        "layer_dims = [2048]*num_layers\n",
        "\n",
        "input_data = np.random.randn(16, layer_dims[0])\n",
        "\n",
        "activations = []\n",
        "\n",
        "weights = []\n",
        "\n",
        "for i in range(num_layers - 1):\n",
        "\n",
        "    ############################################################################\n",
        "    # TODO - Initialize weights using He method for each layer (except the\n",
        "    # last one) and store in variable 'W'\n",
        "    ############################################################################\n",
        "    W= None\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    weights.append(W)\n",
        "\n",
        "# Forward pass through the network\n",
        "for i in range(num_layers):\n",
        "    W = weights[i] if i < num_layers - 1 else None\n",
        "    layer_input = input_data if i == 0 else activations[i - 1]\n",
        "    if W is not None:\n",
        "        layer_output = np.maximum(0, layer_input.dot(W))\n",
        "    else:\n",
        "        layer_output = layer_input\n",
        "    activations.append(layer_output)\n",
        "\n",
        "# Create a figure with subplots for each layer's activation distribution\n",
        "fig, axes = plt.subplots(num_layers, 1, figsize=(8, 12))\n",
        "\n",
        "x_axis = np.arange(-1, 1, 0.01)\n",
        "\n",
        "for i in range(num_layers):\n",
        "    ax = axes[i]\n",
        "    ax.hist(activations[i].flatten(), bins=100, density=True, alpha=0.7, color='blue', label=f'Layer {i+1} Activations')\n",
        "    ax.plot(x_axis, norm.pdf(x_axis, 0, 1), color='red', linestyle='--', label='Standard Normal')\n",
        "    ax.set_title(f'Layer {i+1}')\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CvRw4T3QuAZU",
      "metadata": {
        "id": "CvRw4T3QuAZU"
      },
      "source": [
        "### <font size='4'>**5. Custom Weight Initialization in Pytorch**</font>\n",
        "\n",
        "Custom weight initialization in PyTorch involves setting the weights of a neural network to user-defined values. This can serve different purposes, including enhancing model performance or preventing overfitting.\n",
        "\n",
        "In PyTorch, you can achieve custom weight initialization using the init module, which offers various weight initialization techniques like normal_, uniform_, and kaiming_normal_.\n",
        "\n",
        "Read Documentation for more information - https://pytorch.org/docs/stable/nn.init.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FLOujSxuIZg",
      "metadata": {
        "id": "4FLOujSxuIZg"
      },
      "source": [
        "<font size='4'>**An illustration of custom weight initialization for a neural network model using values sampled from a normal distribution.**</font>\n",
        "\n",
        "```\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    #Initialize weights with values from a normal distribution\n",
        "    def initialize_weights(self):\n",
        "        nn.init.normal_(self.fc1.weight, mean=0, std=0.01)\n",
        "        nn.init.normal_(self.fc2.weight, mean=0, std=0.01)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "model = CustomModel()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f'Parameter name: {name}')\n",
        "        print(param.data)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pNknZRMfj1Li",
      "metadata": {
        "id": "pNknZRMfj1Li"
      },
      "source": [
        "<font size='4' color='Red'>Task 4.6 - Create a class called `Supervise_random_weights` that defines weight using Random Weight Initialization method. (1 point)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-BNTA7uzl6NP",
      "metadata": {
        "id": "-BNTA7uzl6NP"
      },
      "outputs": [],
      "source": [
        "class Supervise_random_weights(nn.Module):\n",
        "    def __init__(self, criterion, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "        for module in self.net.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                ################################################################\n",
        "                # TODO: Set the weights to random values from a normal\n",
        "                # distribution with a mean of 0 and a standard deviation of 0.01\n",
        "                ################################################################\n",
        "                raise NotImplementedError\n",
        "                ################################################################\n",
        "                #                       END OF YOUR CODE                       #\n",
        "                ################################################################\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.net(x).squeeze()\n",
        "        return self.criterion(out, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmS7dGvPrlFW",
      "metadata": {
        "id": "KmS7dGvPrlFW"
      },
      "source": [
        "<font size='4' color='Red'>Task 4.7 - Build a Neural Network architecture as instructed below using the `Supervise_random_weights` class which defines weight Randomly with a mean =0 and std = 0.01 (2 points)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WS3pr7RjpzAp",
      "metadata": {
        "id": "WS3pr7RjpzAp"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(7150)\n",
        "################################################################################\n",
        "# TODO: In this task, we will configure a neural network with Supervise_random_weights\n",
        "# Hidden Dimension = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam - (lr=0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU)-> (Linear + ReLU)->(Linear + ReLU)->\n",
        "# (Linear + ReLU)->(Linear + ReLU)->(Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "################################################################################\n",
        "#                             END OF YOUR CODE                                 #\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4' color='Red'>Task 4.8 - Create a class called `Supervise_Kaiming_weights` that defines weight using Kaiming He's Weight Initialization method. (1 point)</font>\n"
      ],
      "metadata": {
        "id": "8M2-0lkXqRDS"
      },
      "id": "8M2-0lkXqRDS"
    },
    {
      "cell_type": "code",
      "source": [
        "class Supervise_Kaiming_weights(nn.Module):\n",
        "    def __init__(self, criterion, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "\n",
        "        for module in self.net.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                ################################################################\n",
        "                # TODO: Set the weights using He initialization\n",
        "                # [Hint: Python's torch.nn.init.kaiming would be useful]\n",
        "                ################################################################\n",
        "                raise NotImplementedError\n",
        "                ################################################################\n",
        "                #                       END OF YOUR CODE                       #\n",
        "                ################################################################\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.net(x).squeeze()\n",
        "        return self.criterion(out, y)\n"
      ],
      "metadata": {
        "id": "CFUIDVONqM5B"
      },
      "id": "CFUIDVONqM5B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size='4' color='Red'>Task 4.9 - Build a Neural Network architecture as instructed below using the `Supervise_Kaiming_weights` class (2 points)</font>\n"
      ],
      "metadata": {
        "id": "xaRRhLo7rFwt"
      },
      "id": "xaRRhLo7rFwt"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(7150)\n",
        "################################################################################\n",
        "# TODO: In this task, we will configure a neural network with Supervise_He_weights\n",
        "# Hidden Dimension = 32\n",
        "# Loss - Binary Cross Entropy\n",
        "# Optimizer - Adam - (lr=0.01, weight_decay=1e-3)\n",
        "# Network Architecture - (Linear + ReLU)-> (Linear + ReLU)->(Linear + ReLU)->\n",
        "# (Linear + ReLU)->(Linear + ReLU)->(Linear + Sigmoid)\n",
        "################################################################################\n",
        "\n",
        "raise NotImplementedError\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ],
      "metadata": {
        "id": "flOxeeMlrFKA"
      },
      "id": "flOxeeMlrFKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "PwLEuhZxr4qf",
      "metadata": {
        "id": "PwLEuhZxr4qf"
      },
      "source": [
        "**Note** - Any idea how does pytorch initialize weights and biases for a layer by default ? Read this discussion - https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "whTm882GwPLp",
      "metadata": {
        "id": "whTm882GwPLp"
      },
      "source": [
        "## <font size='5'>**Extra Credit Questions**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fiFyXcBIvThZ",
      "metadata": {
        "id": "fiFyXcBIvThZ"
      },
      "source": [
        "<font size='4' color='cyan'>**Extra Credit Question 1:** (2 points)\n",
        "\n",
        "Now try to train a network to classify the data in the file `hard-classification.npz`.  This classification problem is very similar to the original one in `tiny-classification.npz`, with inputs that have a very similar structure.  And yet the problem is harder to learn: do the same training techniques work, or is some other approach necessary?  Hint: consider transfer learning or fine-tuning approaches</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "awkA6vgVwI9b",
      "metadata": {
        "id": "awkA6vgVwI9b"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "miorbotBuox6",
      "metadata": {
        "id": "miorbotBuox6"
      },
      "source": [
        "<font size='4' color='cyan'>**Extra Credit Question 2:** (4 points)\n",
        "\n",
        "One of the most serious drawbacks of deep networks is that, even if they can learn to solve a problem and recognize patterns in the data, they might not give us humans much insight about those solutions.  But if we can create a network that solves a problem, it should be possible to understand that solution. As extra credit, figure out: what classification rule did the neural network learn in the above exercises when the network achieves 100% hold-out accuracy?  Can you extract from the network a succinct set of rules that it implements, for example, can you decompile the network into a short python program, that can correctly assign a class to a sample?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ISqHqxikvSRw",
      "metadata": {
        "id": "ISqHqxikvSRw"
      },
      "source": [
        "$$ \\fbox{Answer: _______________}$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}